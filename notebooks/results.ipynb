{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preamble\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataset import Dataset\n",
    "from model import SVMKNNModel, MLPModel, DualAutoencoderModel\n",
    "from embedder import SVMKNNEmbedder, BinaryEmbedder, GloveEmbedder, SBERTEmbedder\n",
    "from metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDINGS_PATH = '../data/limited_vocab_embeddings_with_special_no_word_token_50d.pt'\n",
    "DATASET_PATH = '../data/trimmed_trials'\n",
    "\n",
    "metrics = Metrics()\n",
    "dataset = Dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiers = [\n",
    "    '',\n",
    "    'slightly',\n",
    "    'greatly',\n",
    "    'smoothly',\n",
    "    'sharply',\n",
    "    'slowly',\n",
    "    'quickly',\n",
    "    'lightly',\n",
    "    'significantly',\n",
    "    'softly',\n",
    "    'harshly',\n",
    "    'gradually',\n",
    "    'immediately',\n",
    "]\n",
    "\n",
    "directions = [\n",
    "    'backward',\n",
    "    'backward down',\n",
    "    'backward left',\n",
    "    'backward right',\n",
    "    'backward up',\n",
    "    'down',\n",
    "    'down forward',\n",
    "    'down left',\n",
    "    'down right',\n",
    "    'forward',\n",
    "    'forward left',\n",
    "    'forward right',\n",
    "    'forward up',\n",
    "    'left',\n",
    "    'left up',\n",
    "    'right',\n",
    "    'right up',\n",
    "    'up',\n",
    "]\n",
    "\n",
    "VOCABULARY = [(f'Move {modifier} {direction}.', np.array([modifier, direction.split(' ')[0], direction.split(' ')[-1] if 'and' in direction else ''], dtype='U16'))\n",
    "              for modifier in modifiers for direction in directions]\n",
    "VOCABULARY += [('', np.array(['', '', ''], dtype='U16'))]\n",
    "\n",
    "def get_results(force_data: NDArray, phrase_data: NDArray, merged_phrase_data: NDArray, exclude_modifier: str = None, exclude_direction: str = None, exclude_composites: bool = False, seed: int = 0, verbose: bool = False, graph_index: int = -1, use_sbert_embedder: bool = False):\n",
    "    if use_sbert_embedder:\n",
    "        embedders = [\n",
    "            SBERTEmbedder(VOCABULARY),\n",
    "            SBERTEmbedder(VOCABULARY)\n",
    "        ]\n",
    "        models = [\n",
    "            MLPModel(embedders[0], phrase_mse_loss=True),\n",
    "            DualAutoencoderModel(embedders[1], phrase_mse_loss=True),\n",
    "        ]\n",
    "\n",
    "        force_datas = [force_data, force_data]\n",
    "        phrase_datas = [merged_phrase_data, merged_phrase_data]\n",
    "        epochs = [1024, 1024]\n",
    "    else:\n",
    "        embedders = [\n",
    "            SVMKNNEmbedder(np.unique(merged_phrase_data), merged_phrase_data.shape[-1]),\n",
    "            BinaryEmbedder(np.unique(merged_phrase_data), merged_phrase_data.shape[-1]),\n",
    "            GloveEmbedder(GLOVE_EMBEDDINGS_PATH, phrase_data.shape[-1]),\n",
    "            BinaryEmbedder(np.unique(merged_phrase_data), merged_phrase_data.shape[-1]),\n",
    "            GloveEmbedder(GLOVE_EMBEDDINGS_PATH, phrase_data.shape[-1]),\n",
    "        ]\n",
    "        models = [\n",
    "            SVMKNNModel(embedders[0]),\n",
    "            MLPModel(embedders[1]),\n",
    "            MLPModel(embedders[2], phrase_mse_loss=True),\n",
    "            DualAutoencoderModel(embedders[3]),\n",
    "            DualAutoencoderModel(embedders[4], phrase_mse_loss=True),\n",
    "        ]\n",
    "\n",
    "        force_datas = [force_data, force_data, force_data, force_data, force_data]\n",
    "        phrase_datas = [merged_phrase_data, merged_phrase_data, phrase_data, merged_phrase_data, phrase_data]\n",
    "        epochs = [0, 1024, 1024, 1024, 1024]\n",
    "\n",
    "    if exclude_modifier is not None:\n",
    "        force_trains, force_tests, phrase_trains, phrase_tests = [], [], [], []\n",
    "        for force_data_, phrase_data_ in zip(force_datas, phrase_datas):\n",
    "            force_trains.append(force_data_[phrase_data[:, 0] != exclude_modifier])\n",
    "            force_tests.append(force_data_[phrase_data[:, 0] == exclude_modifier])\n",
    "            phrase_trains.append(phrase_data_[phrase_data[:, 0] != exclude_modifier])\n",
    "            phrase_tests.append(phrase_data_[phrase_data[:, 0] == exclude_modifier])\n",
    "    elif exclude_direction is not None:\n",
    "        force_trains, force_tests, phrase_trains, phrase_tests = [], [], [], []\n",
    "        for force_data_, phrase_data_ in zip(force_datas, phrase_datas):\n",
    "            force_trains.append(force_data_[phrase_data[:, 1] != exclude_direction])\n",
    "            force_tests.append(force_data_[phrase_data[:, 1] == exclude_direction])\n",
    "            phrase_trains.append(phrase_data_[phrase_data[:, 1] != exclude_direction])\n",
    "            phrase_tests.append(phrase_data_[phrase_data[:, 1] == exclude_direction])\n",
    "    elif exclude_composites:\n",
    "        force_trains, force_tests, phrase_trains, phrase_tests = [], [], [], []\n",
    "        for force_data_, phrase_data_ in zip(force_datas, phrase_datas):\n",
    "            force_trains.append(force_data_[np.char.find(phrase_data[:, 1], ' ') == -1])\n",
    "            force_tests.append(force_data_[np.char.find(phrase_data[:, 1], ' ') != -1])\n",
    "            phrase_trains.append(phrase_data_[np.char.find(phrase_data[:, 1], ' ') == -1])\n",
    "            phrase_tests.append(phrase_data_[np.char.find(phrase_data[:, 1], ' ') != -1])\n",
    "    else:\n",
    "        force_trains, force_tests, phrase_trains, phrase_tests = zip(*[train_test_split(force, phrase, train_size=0.9, random_state=seed) for force, phrase in zip(force_datas, phrase_datas)])\n",
    "\n",
    "    for model, force_train, phrase_train, epoch in zip(models, force_trains, phrase_trains, epochs):\n",
    "        model.train(force_train, phrase_train, epoch, verbose)\n",
    "\n",
    "    force_predictions = [model.phrase_to_force(phrase_test) for model, phrase_test in zip(models, phrase_tests)]\n",
    "    phrase_predictions = [model.force_to_phrase(force_test) for model, force_test in zip(models, force_tests)]\n",
    "\n",
    "    modifier_similarities = [[] for _ in models]\n",
    "    direction_similarities = [[] for _ in models]\n",
    "    curve_shape_acc = [[] for _ in models]\n",
    "    agg_dir_acc = [[] for _ in models]\n",
    "\n",
    "    for i in range(phrase_predictions[0].shape[0]):\n",
    "        if verbose:\n",
    "            print(f'{' '.join(phrase_tests[0][i]).strip():30}', end='')\n",
    "        for j, phrase_prediction in enumerate(phrase_predictions):\n",
    "            modifier_similarity = metrics.score_modifier(phrase_tests[0][i], phrase_prediction[i])\n",
    "            direction_similarity = metrics.score_direction(phrase_tests[0][i], phrase_prediction[i])\n",
    "\n",
    "            modifier_similarities[j].append(modifier_similarity)\n",
    "            direction_similarities[j].append(direction_similarity)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'({modifier_similarity:.4f}) ({direction_similarity:.4f}) {' '.join(phrase_prediction[i]).strip():30}', end='')\n",
    "        if verbose:\n",
    "            print('')\n",
    "    if verbose:\n",
    "        print('')\n",
    "\n",
    "    for i in range(force_predictions[0].shape[0]):\n",
    "        for j, force_prediction in enumerate(force_predictions):\n",
    "            mse = metrics.score_force_profile(force_tests[j][i], force_prediction[i])\n",
    "            dir_sim = metrics.score_force_profile_direction(force_tests[j][i], force_prediction[i])\n",
    "\n",
    "            curve_shape_acc[j].append(mse)\n",
    "            agg_dir_acc[j].append(dir_sim)\n",
    "\n",
    "            if verbose:\n",
    "                print(f'{f'{mse:.4f}':15}', end='')\n",
    "        if verbose:\n",
    "            print('')\n",
    "    if verbose:\n",
    "        print('')\n",
    "\n",
    "    mod_sim = np.mean(np.array(modifier_similarities), axis=1)\n",
    "    dir_sim = np.mean(np.array(direction_similarities), axis=1)\n",
    "    phrase_sim = 0.5 * (mod_sim + dir_sim)\n",
    "    fp_acc = np.mean(np.array(curve_shape_acc), axis=1)\n",
    "    fd_acc = np.mean(np.array(agg_dir_acc), axis=1)\n",
    "    return np.array([mod_sim, dir_sim, phrase_sim, fp_acc, fd_acc])\n",
    "\n",
    "def plot_results(results: NDArray, title: str = '') -> None:\n",
    "    results = results[[3, 4, 0, 1, 2]]\n",
    "\n",
    "    cutoff = 2.0\n",
    "    z_scores = np.apply_along_axis(scipy.stats.zscore, 1, results)\n",
    "    z_scores[0] = -z_scores[0]\n",
    "\n",
    "    model_labels = [\"$SVM/KNN$\", \"$DMLP_B$\", \"$DMLP_S$\", \"$DAE_B$\", \"$DAE_S$\"]\n",
    "    metric_labels = [\"FPAcc\", \"FDAcc\", \"ModSim\", \"DirSim\", \"PhraseSim\"]\n",
    "\n",
    "    plt.figure(figsize=(7, 3.5))\n",
    "    ax = sns.heatmap(\n",
    "        z_scores, vmin=-cutoff, vmax=cutoff, cmap=\"RdYlGn\", annot=results, fmt=\".3f\", linewidths=0.625,\n",
    "        cbar=True, xticklabels=model_labels, yticklabels=metric_labels, annot_kws={\"size\": 16}, cbar_kws={'label': '$z$-score'})\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.tick_params(axis='x', labelsize=14.5)\n",
    "    ax.tick_params(axis='y', labelsize=14.5)\n",
    "    plt.title(title, fontsize=14.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "\n",
    "force_data, phrase_data = dataset.load()\n",
    "merged_phrase_data = dataset.merge_phrase(phrase_data)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "modsim = 0.0\n",
    "dirsim = 0.0\n",
    "\n",
    "for seed in range(N):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    dae = DualAutoencoderModel(SBERTEmbedder(VOCABULARY), phrase_mse_loss=True)\n",
    "\n",
    "    force_train, force_test, phrase_train, phrase_test = train_test_split(force_data, merged_phrase_data, train_size=0.9, random_state=seed)\n",
    "\n",
    "    dae.train(force_train, phrase_train, epochs=1024, verbose=True)\n",
    "\n",
    "    dae_predictions = dae.force_to_phrase(force_test)\n",
    "    print(dae_predictions)\n",
    "    \n",
    "    metrics = Metrics()\n",
    "\n",
    "    for pred, actual in zip(dae_predictions, phrase_test):\n",
    "        pred = [pred.split(' ')[1], pred.split(' ')[-1][:-1] if len(pred.split(' ')) == 3 else pred.split(' ')[-2], pred.split(' ')[-1][:-1] if len(pred.split(' ')) == 4 else '']\n",
    "        actual = [actual.split(' ')[1], actual.split(' ')[-1][:-1] if len(actual.split(' ')) == 3 else actual.split(' ')[-2], actual.split(' ')[-1][:-1] if len(actual.split(' ')) == 4 else '']\n",
    "        modsim += metrics.score_modifier(pred, actual)\n",
    "        dirsim += metrics.score_direction(pred, actual)\n",
    "        counter += 1\n",
    "\n",
    "modsim /= counter\n",
    "dirsim /= counter\n",
    "\n",
    "print(dae_predictions)\n",
    "print(phrase_test)\n",
    "\n",
    "# print(modsim, dirsim, (modsim + dirsim) / 2)\n",
    "# results = np.array([\n",
    "#     [11.714, 0.902, 0.545, 0.982, 0.764],\n",
    "#     [4.523, 0.975, 0.516, 0.978, 0.747],\n",
    "#     [4.700, 0.973, 0.491, 0.928, 0.710],\n",
    "#     [4.454, 0.977, 0.581, 0.979, 0.780],\n",
    "#     [4.582, 0.972, 0.5757338456691258, 0.9344913216692114, 0.7551125836691686],\n",
    "# ]).T[[2, 3, 4, 0, 1]]\n",
    "# plot_results(results, \"Mean Model Scores for In-Distribution Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiers = [\n",
    "    # 'slightly',\n",
    "    'greatly',\n",
    "    # 'smoothly',\n",
    "    # 'sharply',\n",
    "    # 'slowly',\n",
    "    # 'quickly',\n",
    "    # 'lightly',\n",
    "    # 'significantly',\n",
    "    # 'softly',\n",
    "    # 'harshly',\n",
    "    # 'gradually',\n",
    "    # 'immediately',\n",
    "]\n",
    "\n",
    "force_data, phrase_data = dataset.load()\n",
    "merged_phrase_data = dataset.merge_phrase(phrase_data)\n",
    "\n",
    "modsim = 0.0\n",
    "dirsim = 0.0\n",
    "counter = 0\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "for modifier in modifiers:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    dae = MLPModel(SBERTEmbedder(VOCABULARY), phrase_mse_loss=True)#DualAutoencoderModel(SBERTEmbedder(VOCABULARY), phrase_mse_loss=True)\n",
    "\n",
    "    force_train, force_test = force_data[phrase_data[:, 0] != modifier], force_data[phrase_data[:, 0] == modifier]\n",
    "    phrase_train, phrase_test = merged_phrase_data[phrase_data[:, 0] != modifier], merged_phrase_data[phrase_data[:, 0] == modifier]\n",
    "\n",
    "    dae.train(force_train, phrase_train, epochs=1024, verbose=True)\n",
    "\n",
    "    dae_predictions = dae.force_to_phrase(force_test)\n",
    "    print(dae_predictions)\n",
    "\n",
    "    for pred, actual in zip(dae_predictions, phrase_test):\n",
    "        pred = [pred.split(' ')[1], pred.split(' ')[-1][:-1] if len(pred.split(' ')) == 3 else pred.split(' ')[-2], pred.split(' ')[-1][:-1] if len(pred.split(' ')) == 4 else '']\n",
    "        actual = [actual.split(' ')[1], actual.split(' ')[-1][:-1] if len(actual.split(' ')) == 3 else actual.split(' ')[-2], actual.split(' ')[-1][:-1] if len(actual.split(' ')) == 4 else '']\n",
    "        modsim += metrics.score_modifier(pred, actual)\n",
    "        dirsim += metrics.score_direction(pred, actual)\n",
    "        counter += 1\n",
    "\n",
    "modsim /= counter\n",
    "dirsim /= counter\n",
    "\n",
    "print(dae_predictions)\n",
    "print(phrase_test)\n",
    "\n",
    "# print(modsim, dirsim, (modsim + dirsim) / 2)\n",
    "# results = np.array([\n",
    "#     [16.912, 0.787, 0.249, 0.973, 0.611],\n",
    "#     [6.762, 0.976, 0.337, 0.974, 0.655],\n",
    "#     [5.861, 0.956, 0.302, 0.846, 0.574],\n",
    "#     [6.815, 0.978, 0.383, 0.975, 0.679],\n",
    "#     [7.239, 0.935, 0.33380580712109803, 0.9231581677993138, 0.6284819874602059],\n",
    "# ]).T[[2, 3, 4, 0, 1]]\n",
    "# plot_results(results, \"Model Scores on Out-of-Distribution Modifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\n",
    "    # 'backward',\n",
    "    # 'backward down',\n",
    "    # 'backward left',\n",
    "    # 'backward right',\n",
    "    'backward up',\n",
    "    # 'down',\n",
    "    # 'down forward',\n",
    "    # 'down left',\n",
    "    # 'down right',\n",
    "    # 'forward',\n",
    "    # 'forward left',\n",
    "    # 'forward right',\n",
    "    # 'forward up',\n",
    "    # 'left',\n",
    "    # 'left up',\n",
    "    # 'right',\n",
    "    # 'right up',\n",
    "    # 'up',\n",
    "]\n",
    "\n",
    "force_data, phrase_data = dataset.load()\n",
    "merged_phrase_data = dataset.merge_phrase(phrase_data)\n",
    "merged_phrase_data2 = dataset.merge_directions(phrase_data)\n",
    "\n",
    "modsim = 0.0\n",
    "dirsim = 0.0\n",
    "counter = 0\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "for direction in directions:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    dae = DualAutoencoderModel(SBERTEmbedder(VOCABULARY), phrase_mse_loss=True)\n",
    "\n",
    "    force_train, force_test = force_data[merged_phrase_data2[:, 1] != direction], force_data[merged_phrase_data2[:, 1] == direction]\n",
    "    phrase_train, phrase_test = merged_phrase_data[merged_phrase_data2[:, 1] != direction], merged_phrase_data[merged_phrase_data2[:, 1] == direction]\n",
    "\n",
    "    dae.train(force_train, phrase_train, epochs=1024, verbose=True)\n",
    "\n",
    "    dae_predictions = dae.force_to_phrase(force_test)\n",
    "\n",
    "    for pred, actual in zip(dae_predictions, phrase_test):\n",
    "        pred = [pred.split(' ')[1], pred.split(' ')[-1][:-1] if len(pred.split(' ')) == 3 else pred.split(' ')[-2], pred.split(' ')[-1][:-1] if len(pred.split(' ')) == 4 else '']\n",
    "        actual = [actual.split(' ')[1], actual.split(' ')[-1][:-1] if len(actual.split(' ')) == 3 else actual.split(' ')[-2], actual.split(' ')[-1][:-1] if len(actual.split(' ')) == 4 else '']\n",
    "        modsim += metrics.score_modifier(pred, actual)\n",
    "        dirsim += metrics.score_direction(pred, actual)\n",
    "        counter += 1\n",
    "\n",
    "modsim /= counter\n",
    "dirsim /= counter\n",
    "\n",
    "print(dae_predictions)\n",
    "print(phrase_test)\n",
    "\n",
    "# print(modsim, dirsim, (modsim + dirsim) / 2)\n",
    "# results = np.array([\n",
    "#     [21.749, 0.449, 0.471, 0.648, 0.560],\n",
    "#     [25.697, 0.044, 0.453, 0.626, 0.540],\n",
    "#     [11.515, 0.789, 0.491, 0.667, 0.579],\n",
    "#     [31.103, -0.222, 0.489, 0.607, 0.548],\n",
    "#     [9.269, 0.869, 0.5200380712392785, 0.6344676013503756, 0.577252836294827],\n",
    "# ]).T[[2, 3, 4, 0, 1]]\n",
    "# plot_results(results, \"Model Scores on Out-of-Distribution Directions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = results[[3, 4, 0, 1, 2]]\n",
    "\n",
    "#     cutoff = 2.0\n",
    "#     z_scores = np.apply_along_axis(scipy.stats.zscore, 1, results)\n",
    "#     z_scores[0] = -z_scores[0]\n",
    "\n",
    "#     model_labels = [\"$SVM/KNN$\", \"$DMLP_B$\", \"$DMLP_G$\", \"$DAE_B$\", \"$DAE_G$\"]\n",
    "#     metric_labels = [\"FPAcc\", \"FDAcc\", \"ModSim\", \"DirSim\", \"PhraseSim\"]\n",
    "\n",
    "#     plt.figure(figsize=(7, 3.5))\n",
    "#     ax = sns.heatmap(\n",
    "#         z_scores, vmin=-cutoff, vmax=cutoff, cmap=\"RdYlGn\", annot=results, fmt=\".3f\", linewidths=0.625,\n",
    "#         cbar=True, xticklabels=model_labels, yticklabels=metric_labels, annot_kws={\"size\": 16}, cbar_kws={'label': '$z$-score'})\n",
    "#     ax.xaxis.set_label_position('bottom')\n",
    "#     ax.tick_params(axis='x', labelsize=14.5)\n",
    "#     ax.tick_params(axis='y', labelsize=14.5)\n",
    "#     plt.title(title, fontsize=14.5)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "\n",
    "force_data, phrase_data = dataset.load()\n",
    "merged_phrase_data = dataset.merge_directions(phrase_data)\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for seed in range(N):\n",
    "    torch.manual_seed(seed)\n",
    "    results = get_results(force_data, phrase_data, merged_phrase_data, seed=seed, verbose=False)\n",
    "    final_results.append(results)\n",
    "    plot_results(results, f\"Model Scores for In-Distribution Samples (Seed {seed})\")\n",
    "\n",
    "plot_results(np.mean(final_results, axis=0), \"Mean Model Scores for In-Distribution Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiers = [\n",
    "    'slightly',\n",
    "    'greatly',\n",
    "    'smoothly',\n",
    "    'sharply',\n",
    "    'slowly',\n",
    "    'quickly',\n",
    "    'lightly',\n",
    "    'significantly',\n",
    "    'softly',\n",
    "    'harshly',\n",
    "    'gradually',\n",
    "    'immediately',\n",
    "]\n",
    "\n",
    "force_data, phrase_data = dataset.load()\n",
    "merged_phrase_data = dataset.merge_directions(phrase_data)\n",
    "\n",
    "modifier_results = {}\n",
    "\n",
    "for modifier in modifiers:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    N = 1\n",
    "    modifier_results[modifier] = 0.0\n",
    "\n",
    "    for _ in range(N):\n",
    "        modifier_results[modifier] += get_results(force_data, phrase_data, merged_phrase_data, exclude_modifier=modifier, verbose=False)\n",
    "\n",
    "    modifier_results[modifier] /= N\n",
    "    plot_results(modifier_results[modifier], f\"Model Scores on Out-of-Distribution Modifiers ('{modifier}')\")\n",
    "\n",
    "plot_results(np.mean([modifier_results[modifier] for modifier in modifier_results], axis=0), \"Model Scores on Out-of-Distribution Modifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [\n",
    "    'backward',\n",
    "    'backward down',\n",
    "    'backward left',\n",
    "    'backward right',\n",
    "    'backward up',\n",
    "    'down',\n",
    "    'down forward',\n",
    "    'down left',\n",
    "    'down right',\n",
    "    'forward',\n",
    "    'forward left',\n",
    "    'forward right',\n",
    "    'forward up',\n",
    "    'left',\n",
    "    'left up',\n",
    "    'right',\n",
    "    'right up',\n",
    "    'up',\n",
    "]\n",
    "\n",
    "force_data, phrase_data = dataset.load()\n",
    "merged_phrase_data = dataset.merge_directions(phrase_data)\n",
    "\n",
    "direction_results = {}\n",
    "\n",
    "for direction in directions:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    N = 1\n",
    "    direction_results[direction] = 0.0\n",
    "\n",
    "    for _ in range(N):\n",
    "        direction_results[direction] += get_results(force_data, phrase_data, merged_phrase_data, exclude_direction=direction, verbose=False)\n",
    "\n",
    "    direction_results[direction] /= N\n",
    "    plot_results(direction_results[direction], f\"Model Scores on Out-of-Distribution Directions ('{direction}')\")\n",
    "\n",
    "plot_results(np.mean([direction_results[direction] for direction in direction_results], axis=0), \"Model Scores on Out-of-Distribution Directions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
